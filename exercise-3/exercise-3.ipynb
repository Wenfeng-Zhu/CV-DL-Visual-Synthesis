{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "## Exercise-3\n",
    "\n",
    "Wenfeng Zhu-Computer Vision and Deep Learning: Visual Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-1: Analytical Gradients of the Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The derivation of analytical gradient of the cross entropy loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three functions will be used in the derivation:\n",
    "\n",
    "1. Cross entropy for single sample\\\n",
    "$$\n",
    "\\mathcal{L}^i = -log[\\sigma(Wx^i)_{y^i}]\n",
    "$$\n",
    "\n",
    "2. Softmax function\\\n",
    "$$\n",
    "\\sigma\\mathcal(z)_i = \\frac {e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "3. Kronecker-delta\n",
    "$$\n",
    "\\delta_{mn}=\n",
    "\\begin{cases}\n",
    "1, &m=n\\\\\n",
    "0, &m \\neq n\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "For the analytical gradient of the cross entropy loss:\\\n",
    "$$\n",
    "\\frac {\\partial\\mathcal{L}^i} {W_{l,k}} = \\frac {\\partial\\mathcal{L}^i} {\\partial p^i_l} * \\frac {\\partial p^i_l} {\\partial s^i_l} * \\frac {\\partial s^i_l} {\\partial W_{l,k}}\n",
    "$$\n",
    "\n",
    "Among them: \\\n",
    "$ p^i = \\sigma{(s^i_l)} $ is the probability of $x^i$ after normalizing the score function by softmax\\\n",
    "$ s^i = Wx^i $ is the score function\\\n",
    "$ s^i_l$ is the score of a certain label\n",
    "\n",
    "For the first item:\\\n",
    "$$\n",
    "\\mathcal{L}^i = -log(p^i_l)   ====>  \\frac {\\partial\\mathcal{L}^i} {\\partial p^i_l} = -\\frac 1 {p^i_l} \n",
    "$$\n",
    "For the second item,$p^i_l$ is:\\\n",
    "$$\n",
    "p^i = \\sigma(s^i) = \\frac {e^{s^i_l}}{\\sum e^{s^i}}\n",
    "$$\n",
    "Derive this equation: \n",
    "$$\n",
    "\\frac {\\partial p^i} {\\partial s^i_l} = \\frac {\\frac {\\partial e^{s^i}} {\\partial s^i_l}*\\sum e^{s^i}-e^{s^i_l}*\\frac {\\partial \\sum e^{s^i}} {\\partial s^i_l}}{(\\sum e^{s^i})^2}=\n",
    "\\begin{cases}\n",
    "\\frac {e^{s^i_l}}{\\sum e^{s^i}}*(1-\\frac {e^{s^i_l}}{\\sum e^{s^i}}) = p^i_l*(1-p^i_l), &\\mathcal l\\text{ correspond to the real label}\\\\\n",
    "\\frac {e^{s^i_l}}{\\sum e^{s^i}}*\\frac {e^{s^i_m}}{\\sum e^{s^i}}= p^i_l*(0-p^i_l), &\\mathcal l\\text{ don't correspond to the real label}\n",
    "\\end{cases}\n",
    "$$\n",
    "For the third item, about scores function, it's a linear function of the input, so we can get:\\\n",
    "$$\n",
    "\\frac {\\partial s^{lk}} {\\partial W_{l,k}} = x^i_k\n",
    "$$\n",
    "\n",
    "In summary:\n",
    "$$\n",
    "\\frac {\\partial\\mathcal{L}^i} {W_{l,k}} = \\frac {\\partial\\mathcal{L}^i} {\\partial p^i_l} * \\frac {\\partial p^i_l} {\\partial s^i_l} * \\frac {\\partial s^i_l} {\\partial W_{l,k}} = x^i_k*(p^i_l-\\delta_{ly^i}) = x^i_k*(\\sigma(Wx^i)_l-\\delta_{ly^i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Data Preparation and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "1. **Import the necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from tqdm.auto import tqdm  # Not needed but very cool!\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def load_data(train=True):\n",
    "    mnist = datasets.MNIST('../data',\n",
    "                           train=train,\n",
    "                           download=True)\n",
    "    return mnist\n",
    "\n",
    "\n",
    "def plot_examples(data):\n",
    "    #########################\n",
    "    #### Your Code here  ####\n",
    "    #########################\n",
    "    # print(random.randint(0, 5996))\n",
    "    train_numpy = data.data.numpy()\n",
    "    # print(np.argmax(train_numpy, axis=0))\n",
    "    print(\"The max of train dataset is:\\n\", train_numpy.max(axis=0))\n",
    "    print(\"\\nThe min of train dataset is:\\n\", train_numpy.min(axis=0))\n",
    "    print(\"\\nThe mean of train dataset is:\\n\", train_numpy.mean(axis=0))\n",
    "    print(\"\\nThe shape of train dataset is:\\n\", data.data.shape)\n",
    "    print(\"\\nThe dtype of train dataset is:\\n\", data.data.type())\n",
    "    # Plot some examples and put their corresponding label on top as title.\n",
    "    start = random.randint(0, 59989)\n",
    "    for i in range(10):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(data.data[start + i].numpy(), cmap=\"gray\")\n",
    "        plt.title(data.targets[i+start])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "data = load_data()\n",
    "# subtask 1\n",
    "plot_examples(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Convert all images into plain vectors and process them to be centered around 0 in the range of [-1, 1]. In the end you should have two arrays of images and labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def convert_mnist_to_vectors(data):\n",
    "    '''Converts the ``[28, 28]`` MNIST images to vectors of size ``[28*28]``.\n",
    "       It outputs mnist_vectors as a array with the shape of [N, 784], where\n",
    "       N is the number of images in data.\n",
    "    '''\n",
    "\n",
    "    mnist_vectors = []\n",
    "    labels = []\n",
    "\n",
    "    #########################\n",
    "    #### Your Code here  ####\n",
    "    #########################\n",
    "    # image--PIL.Image.Image; label--int\n",
    "    for image, label in tqdm(data):\n",
    "        mnist_vectors.append(np.asarray(image).ravel())\n",
    "        labels.append(label)\n",
    "\n",
    "    # return as numpy arrays\n",
    "    mnist_vectors = np.asarray(mnist_vectors)\n",
    "    mnist_vectors_center = (mnist_vectors - mnist_vectors.mean(axis=1)[:, None]) / (\n",
    "            mnist_vectors.max(axis=1)[:, None] - mnist_vectors.min(axis=1)[:, None])\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    return mnist_vectors_center, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Now run the provided do_pca on the converted data in order to obtain a matrix of sorted eigenvectors that represent the principal components of the train set. Reshape the 10 most important principal components to the shape of [28, 28] in order to plot them as images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def do_pca(data):\n",
    "    '''Returns matrix [784x784] whose columns are the sorted eigenvectors.\n",
    "       Eigenvectors (prinicipal components) are sorted according to their\n",
    "       eigenvalues in decreasing order.\n",
    "    '''\n",
    "\n",
    "    mnist_vectors, labels = convert_mnist_to_vectors(data)\n",
    "    #     prepare_data(mnist_vectors)\n",
    "\n",
    "    # compute covariance matrix of data with shape [784x784]\n",
    "    cov = np.cov(mnist_vectors.T)\n",
    "    # print(\"covariance matrix\",cov[390])\n",
    "\n",
    "    # compute eigenvalues and vectors\n",
    "    eigVals, eigVec = np.linalg.eig(cov)\n",
    "\n",
    "    # sort eigenVectors by eigenValues\n",
    "    sorted_index = eigVals.argsort()[::-1]\n",
    "    eigVals = eigVals[sorted_index]\n",
    "    sorted_eigenVectors = eigVec[:, sorted_index]\n",
    "    print(type(sorted_eigenVectors), sorted_eigenVectors.shape)\n",
    "    sorted_eigenVectors_real = sorted_eigenVectors.real.astype(float).T\n",
    "    return sorted_eigenVectors.real.astype(float).T\n",
    "\n",
    "\n",
    "def plot_pcs(sorted_eigenVectors, num=10):\n",
    "    '''Plots the first ``num`` eigenVectors as images.'''\n",
    "\n",
    "    #########################\n",
    "    #### Your Code here  ####\n",
    "    #########################\n",
    "    pc_10 = np.empty((10, 28, 28))\n",
    "    for i in range(num):\n",
    "        pc_10[i] = sorted_eigenVectors[i].reshape((28, 28))\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(sorted_eigenVectors[i].reshape((28, 28)), cmap=\"gray\")\n",
    "        plt.title(\"PC-\"+str(i+1))\n",
    "    plt.show()    \n",
    "\n",
    "# # subtask 3\n",
    "pcs = do_pca(data)\n",
    "#\n",
    "# # subtask 3\n",
    "plot_pcs(pcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Explain what you are seeing. Can you make a statement regarding the difficulty of MNIST digit classification problem? What would you expect the principal components to look like, if the problem was easy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ten most significant principal components derived from PCA are finally shown, and basically possess a distribution similar to the actual label classification.\\\n",
    "About MNIST digit classification problem. It is not difficult because the distribution of graphs corresponding to different label classes in most of training datas has significant differences, It makes the principal components obtained by PCA well summarized.\\\n",
    "I actually expected to see the principal components that could correspond to the actual labels being displayed. However, the meanings of the individual feature dimensions of the principal components of PCA are somewhat ambiguous and not as explanatory as the original sample features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Project the MNIST vectors of the train set onto the two most important principal components (associated with two largest eigenvalues). Use the dot product for the projection into the 2D feature space spanned by the two principal components and plot the resulting points in a scatter (use the scatter provided by matplotlib for this) plot. To get a better overview you can also choose a subset of the points. Color each dot corresponding to its class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_projection(sorted_eigenVectors, data):\n",
    "    '''Projects ``data`` onto the first two ``sorted_eigenVectors`` and makes\n",
    "    a scatterplot of the resulting points'''\n",
    "\n",
    "    #########################\n",
    "    #### Your Code here  ####\n",
    "    #########################\n",
    "    mnist_vectors, labels = convert_mnist_to_vectors(data)\n",
    "    pc_1 = mnist_vectors @ sorted_eigenVectors[0]\n",
    "    pc_2 = mnist_vectors @ sorted_eigenVectors[1]\n",
    "    for i in range(10):\n",
    "        indices = np.argwhere(labels == i).ravel()\n",
    "        # print(indices.shape, pc_1.shape)\n",
    "        plt.scatter(pc_1, pc_2)\n",
    "        plt.scatter(pc_1[indices], pc_2[indices], color=\"red\")\n",
    "        plt.xlabel(\"PC-1\")\n",
    "        plt.ylabel(\"PC-2\")\n",
    "        plt.title(\"Projection from MNIST to 1&2-PC feature space(Red with label-\" + str(i)+\")\")\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "plot_projection(pcs, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Interpret the plot. What can it tell us about the MNIST dataset? Can you make a statement regarding the difficulty of MNIST digit classification problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots show the distribution of the MNIST dataset in PC-1&PC-2 feature space. The degree of dispersion reflects the interpretability of this feature space. And obviously, the feature space consisting of principal component 1 and principal component 2 is not good enough in terms of discrete separation for all labels. That is, this feature space loses a certain amount of information. Back to the MNIST classification problem, when applying the PCA method, the difficulty lies in selection of the number of dimensions for feature space, too many feature dimensions will cause an increase in computation and too few feature dimensions will cause a loss of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Defining, Training and Evaluating an MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "1. **Using all skeleton functions provided for this task, build a 5-layer neural network, which accepts MNIST vectors (those from task 2) as input and outputs for each MNIST vector a classification vector containing 10 values, one for each MNIST class. Each hidden layer of the network has 100 hidden units (output feature dimension). All hidden layers should use ReLU activations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "\n",
    "    def __init__(self, size_hidden=100, size_out=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(28 * 28, size_hidden)\n",
    "        self.fc2 = nn.Linear(size_hidden, size_hidden)\n",
    "        self.fc3 = nn.Linear(size_hidden, size_hidden)\n",
    "        self.fc4 = nn.Linear(size_hidden, size_hidden)\n",
    "        self.out_layer = nn.Linear(size_hidden, size_out)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # Your Code here: The rest of the layers\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.out_layer(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Implement a function, which can report the accuracy of a batch of predictions in percent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(prediction, label):\n",
    "    return torch.count_nonzero(prediction == label) / prediction.shape[0] * 100\n",
    "\n",
    "\n",
    "def class_label(prediction):\n",
    "    return torch.argmax(prediction, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Train the network for at least 5 epochs and validate the classifier on the test split of the data after each epoch. Do so using the ADAM optimizer from pytorch’s optim package and set the learning rate to 1×10^-4 . Use the Cross Entropy loss provided by pytorch as loss function and then update the network weights using backpropagation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ecf806052034d98828d2edcb69b037e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37574d0c5fe4f338386a3125aa0b1d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da04007e173440ca6074353dc10cd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e62679a9bf746d2b8f6537b8b730886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In step- 0\n",
      "Batch Accuracy: 6.25%, Loss: 2.302455186843872\n",
      "In step- 375\n",
      "Batch Accuracy: 68.75%, Loss: 0.9474689364433289\n",
      "In step- 750\n",
      "Batch Accuracy: 56.25%, Loss: 0.7254382967948914\n",
      "In step- 1125\n",
      "Batch Accuracy: 87.5%, Loss: 0.364238440990448\n",
      "In step- 1500\n",
      "Batch Accuracy: 75.0%, Loss: 0.7021346092224121\n",
      "In step- 1875\n",
      "Batch Accuracy: 93.75%, Loss: 0.22982126474380493\n",
      "In step- 2250\n",
      "Batch Accuracy: 75.0%, Loss: 0.6005139946937561\n",
      "In step- 2625\n",
      "Batch Accuracy: 75.0%, Loss: 0.9303790330886841\n",
      "In step- 3000\n",
      "Batch Accuracy: 93.75%, Loss: 0.24704250693321228\n",
      "In step- 3375\n",
      "Batch Accuracy: 81.25%, Loss: 0.5365638136863708\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5514560b3ceb4600ad67a7ad8a34e775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 90.6500015258789%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23ab8d9d53148d6aa124d95f057a4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In step- 0\n",
      "Batch Accuracy: 75.0%, Loss: 0.6298738121986389\n",
      "In step- 375\n",
      "Batch Accuracy: 100.0%, Loss: 0.0898246020078659\n",
      "In step- 750\n",
      "Batch Accuracy: 81.25%, Loss: 0.5346322655677795\n",
      "In step- 1125\n",
      "Batch Accuracy: 100.0%, Loss: 0.07693550735712051\n",
      "In step- 1500\n",
      "Batch Accuracy: 100.0%, Loss: 0.11610755324363708\n",
      "In step- 1875\n",
      "Batch Accuracy: 93.75%, Loss: 0.124869704246521\n",
      "In step- 2250\n",
      "Batch Accuracy: 81.25%, Loss: 0.4926328957080841\n",
      "In step- 2625\n",
      "Batch Accuracy: 87.5%, Loss: 0.21239249408245087\n",
      "In step- 3000\n",
      "Batch Accuracy: 93.75%, Loss: 0.16455663740634918\n",
      "In step- 3375\n",
      "Batch Accuracy: 93.75%, Loss: 0.12448021024465561\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e9dbd23d6d4aa0b7c17c8178b4a161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 93.05999755859375%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9863c93f5d034f2eb409bbdf4092d5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In step- 0\n",
      "Batch Accuracy: 100.0%, Loss: 0.03902246430516243\n",
      "In step- 375\n",
      "Batch Accuracy: 93.75%, Loss: 0.24500279128551483\n",
      "In step- 750\n",
      "Batch Accuracy: 100.0%, Loss: 0.08635013550519943\n",
      "In step- 1125\n",
      "Batch Accuracy: 93.75%, Loss: 0.25489750504493713\n",
      "In step- 1500\n",
      "Batch Accuracy: 100.0%, Loss: 0.031160522252321243\n",
      "In step- 1875\n",
      "Batch Accuracy: 93.75%, Loss: 0.16602067649364471\n",
      "In step- 2250\n",
      "Batch Accuracy: 100.0%, Loss: 0.08177223056554794\n",
      "In step- 2625\n",
      "Batch Accuracy: 93.75%, Loss: 0.09100216627120972\n",
      "In step- 3000\n",
      "Batch Accuracy: 100.0%, Loss: 0.1029285341501236\n",
      "In step- 3375\n",
      "Batch Accuracy: 100.0%, Loss: 0.07787588983774185\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b36a4df3b1446785950b22c084fa3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 94.1500015258789%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6404184f8a4198abc1d0e9d56fc983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In step- 0\n",
      "Batch Accuracy: 93.75%, Loss: 0.11674240231513977\n",
      "In step- 375\n",
      "Batch Accuracy: 100.0%, Loss: 0.02061454765498638\n",
      "In step- 750\n",
      "Batch Accuracy: 100.0%, Loss: 0.02933359518647194\n",
      "In step- 1125\n",
      "Batch Accuracy: 93.75%, Loss: 0.4323282539844513\n",
      "In step- 1500\n",
      "Batch Accuracy: 93.75%, Loss: 0.48027607798576355\n",
      "In step- 1875\n",
      "Batch Accuracy: 93.75%, Loss: 0.13841146230697632\n",
      "In step- 2250\n",
      "Batch Accuracy: 93.75%, Loss: 0.1755780428647995\n",
      "In step- 2625\n",
      "Batch Accuracy: 93.75%, Loss: 0.18431848287582397\n",
      "In step- 3000\n",
      "Batch Accuracy: 100.0%, Loss: 0.023054376244544983\n",
      "In step- 3375\n",
      "Batch Accuracy: 100.0%, Loss: 0.06282919645309448\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a81ce75e7e84efa9265133a9b48ea57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 94.93000030517578%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc39ed0fb7349fdaa783e1cc32d5081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In step- 0\n",
      "Batch Accuracy: 93.75%, Loss: 0.1846465766429901\n",
      "In step- 375\n",
      "Batch Accuracy: 100.0%, Loss: 0.058858610689640045\n",
      "In step- 750\n",
      "Batch Accuracy: 93.75%, Loss: 0.0835513174533844\n",
      "In step- 1125\n",
      "Batch Accuracy: 87.5%, Loss: 0.3890409469604492\n",
      "In step- 1500\n",
      "Batch Accuracy: 100.0%, Loss: 0.022243686020374298\n",
      "In step- 1875\n",
      "Batch Accuracy: 87.5%, Loss: 0.5894659757614136\n",
      "In step- 2250\n",
      "Batch Accuracy: 93.75%, Loss: 0.287882000207901\n",
      "In step- 2625\n",
      "Batch Accuracy: 93.75%, Loss: 0.11984382569789886\n",
      "In step- 3000\n",
      "Batch Accuracy: 93.75%, Loss: 0.15756013989448547\n",
      "In step- 3375\n",
      "Batch Accuracy: 93.75%, Loss: 0.3647814691066742\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a99c4f8bf7b4c0ea2a5cabfa720ad40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 95.18000030517578%\n"
     ]
    }
   ],
   "source": [
    "class MnistVectors(torch.utils.data.Dataset):\n",
    "    '''A Pytorch Dataset, which does the same data preparation as was done in\n",
    "    the PCA exercise.'''\n",
    "\n",
    "    def __init__(self, split='train'):\n",
    "        super().__init__()\n",
    "\n",
    "        mnist = datasets.MNIST('../data',\n",
    "                               train=split == 'train',\n",
    "                               download=True)\n",
    "\n",
    "        ########################\n",
    "        #### Your Code here ####\n",
    "        # self.mnist_vectors, self.labels = your conversion function from task 2\n",
    "        ########################\n",
    "        self.mnist_vectors, self.labels = convert_mnist_to_vectors(mnist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''Implements the ``[idx]`` method. Here we convert the numpy data to\n",
    "        torch tensors.\n",
    "        '''\n",
    "\n",
    "        mvec = torch.tensor(self.mnist_vectors[idx]).float()\n",
    "        label = torch.tensor(self.labels[idx]).long()\n",
    "\n",
    "        return mvec, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    \n",
    "def train(use_gpu=True):  # if torch.cuda.is_available(), use gpu to speed up training\n",
    "\n",
    "    # Here we instantiate our model. The weights of the model are automatically\n",
    "    # initialized by pytorch\n",
    "    P = MultilayerPerceptron()\n",
    "    # print(P.parameters())\n",
    "\n",
    "    TrainData = MnistVectors()\n",
    "    TestData = MnistVectors('test')\n",
    "    # Dataloaders allow us to load the data in batches. This allows us a better\n",
    "    # estimate of the parameter updates when doing backprop.\n",
    "    # We need two Dataloaders so that we can train on the train data split\n",
    "    # and evaluate on the test datasplit.\n",
    "    Dl = DataLoader(TrainData, batch_size=16, shuffle=True)\n",
    "    testDl = DataLoader(TestData, batch_size=16, shuffle=False)\n",
    "\n",
    "    # Use the Adam optimizer with learning rate 1e-4 and otherwise default value\n",
    "    # Use the torch.optim.Adam to define a optimizer, use the parameters() method of MLP to pass in iterable parameters and define the leaning rate with 1e-4\n",
    "    optimizer = torch.optim.Adam(P.parameters(), lr=1e-4)\n",
    "\n",
    "    # Use the Cross Entropy loss from pytorch. Make sure your MultilayerPerceptron does not use any activation function on the output layer! (Do you know why?)\n",
    "    criterion = nn.CrossEntropyLoss()  # define a loss function with Cross Entropy loss\n",
    "    print(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    if use_gpu:\n",
    "        P.cuda()\n",
    "        criterion.cuda()\n",
    "\n",
    "    for epoch in tqdm(range(5), desc='Epoch'):\n",
    "        for step, [example, label] in enumerate(tqdm(Dl, desc='Batch')):\n",
    "            if use_gpu:\n",
    "                example = example.cuda()\n",
    "                label = label.cuda()\n",
    "\n",
    "            # The optimizer knows about all model parameters. These in turn\n",
    "            # store their own gradients. When calling loss.backward() the newly\n",
    "            # computed gradients are added on top of the existing ones. Thus\n",
    "            # at before calculating new gradients we need to clear the old ones\n",
    "            # using ther zero_grad() method.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            prediction = P(example)\n",
    "\n",
    "            loss = criterion(prediction, label)\n",
    "\n",
    "            # Here pytorch applies backpropagation for us completely automatically!!! That is quite awesome!\n",
    "            loss.backward()\n",
    "\n",
    "            # The step method now adds the gradients onto the model parameters as specified by the optimizer and the learning rate.\n",
    "            optimizer.step()\n",
    "\n",
    "            # To keep track of what is happening print some outputs from time to time.\n",
    "            if (step % 375) == 0:\n",
    "                # Your code here\n",
    "                print(\"In step-\", step)\n",
    "                acc = batch_accuracy(class_label(prediction), label)\n",
    "                tqdm.write('Batch Accuracy: {}%, Loss: {}'.format(acc, loss))\n",
    "\n",
    "        # Now validate on the whole test set\n",
    "        accuracies = []\n",
    "        for idx, [test_ex, test_l] in enumerate(tqdm(testDl, desc='Test')):\n",
    "            if use_gpu:\n",
    "                test_ex = test_ex.cuda()\n",
    "                test_l = test_l.cuda()\n",
    "\n",
    "            #########################\n",
    "            #### Your Code here  ####\n",
    "            #########################\n",
    "            accuracies.append(batch_accuracy(class_label(P(test_ex)), test_l).cpu().numpy())\n",
    "\n",
    "            # Using your batch_accuracy function, also print the mean accuracy\n",
    "            # over the whole test split of the data.\n",
    "\n",
    "        print('Validation Accuracy: {}%'.format(np.mean(accuracies)))\n",
    "        \n",
    "        \n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **How well does your model perform on the test data after training?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
